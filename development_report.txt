
Model Training and Evaluation Report

Overview of the Approach

This report summarizes the approach taken to train a YOLOv8 model for object segmentation using a custom dataset sourced from YouTube videos. The dataset was created by breaking down selected videos into frames, which generated multiple images for subsequent annotation. The YOLOv8n-seg.pt model was chosen for its balance between accuracy and speed, making it suitable for real-time video processing tasks.

Data Collection and Annotation
The initial step involved sourcing a dataset from YouTube videos. Selected videos were downloaded and processed to extract frames, creating a diverse set of images that represent the target objects effectively. The dataset comprised a total of 100 images, providing sufficient variation in object appearance, scale, and environmental context.

To annotate these images, the open-source tool LabelMe was utilized. This tool facilitated the manual labeling of objects within each frame, producing annotations in JSON format. After annotation, the JSON files were converted into text files and associated images, ready for training. This conversion process allowed for compatibility with the YOLO model requirements, ensuring the annotations correctly mapped to their corresponding images.

Model Selection and Training
The YOLOv8n-seg.pt model was selected for training due to its state-of-the-art performance in object detection and segmentation tasks. This model is particularly well-suited for real-time applications, which is crucial given the projectâ€™s video processing objectives. The training process was conducted using the train.py script provided with the YOLOv8 framework.

For training, several arguments were specified in the training script to optimize performance, including:

Batch Size: A batch size of 16 was utilized to balance memory usage and training speed.
Learning Rate: An initial learning rate of 0.001 was set, which could be adjusted dynamically during training to improve convergence.
Epochs: The model was trained over 100 epochs to ensure adequate learning from the dataset.
Data Augmentation: Various augmentation techniques, such as random flipping and scaling, were applied to enhance model robustness against overfitting.
Results and Evaluation
Post-training, the results were saved in the "Train" folder, which contains the model weights and logs of the training process. The model was evaluated using the predict.py script on a separate test video, yielding an output video demonstrating the model's segmentation capabilities.

Performance Metrics
The performance of the model was assessed using standard evaluation metrics, including:

Mean Average Precision (mAP): This metric assesses the accuracy of object detection across different classes, providing insights into the overall effectiveness of the model.
IoU (Intersection over Union): IoU was calculated for each detected object to evaluate the overlap between predicted and ground truth bounding boxes.

Insights and Future Work
The model's ability to accurately segment objects from the video demonstrates the effectiveness of the chosen approach. However, there are areas for potential improvement:

Dataset Expansion: Increasing the dataset size and diversity by including more frames and varying conditions could enhance model robustness.
Fine-tuning: Further tuning of hyperparameters and experimenting with different augmentation strategies may yield improved performance.
Real-time Testing: Implementing the model in real-time scenarios will provide practical insights into its performance and applicability.


In conclusion, the YOLOv8n-seg model trained on the curated dataset has shown promising results in object segmentation tasks, paving the way for further developments and applications in video processing. Users can replicate the training process using the provided best.pt file from the weights folder to run the model on other videos, facilitating broader experimentation and use cases.